{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "type": "object",
    "title": "Voxtral HF Configuration",
    "properties": {
        "model_id": {
            "type": "string",
            "enum": [
                "mistralai/Voxtral-Mini-3B-2507",
                "mistralai/Voxtral-Small-24B-2507"
            ],
            "default": "mistralai/Voxtral-Mini-3B-2507",
            "description": "Voxtral model to use. Mini is faster, Small is more accurate."
        },
        "device": {
            "type": "string",
            "enum": [
                "auto",
                "cpu",
                "cuda"
            ],
            "default": "auto",
            "description": "Device for inference (auto will use CUDA if available)"
        },
        "dtype": {
            "type": "string",
            "enum": [
                "auto",
                "bfloat16",
                "float16",
                "float32"
            ],
            "default": "auto",
            "description": "Data type for model weights (auto will use bfloat16 on GPU, float32 on CPU)"
        },
        "language": {
            "type": [
                "string",
                "null"
            ],
            "default": "en",
            "description": "Language code for transcription (e.g., 'en', 'es', 'fr')",
            "examples": [
                "ar",
                "nl",
                "en",
                "fr",
                "de",
                "hi",
                "it",
                "pt",
                "es"
            ]
        },
        "max_new_tokens": {
            "type": "integer",
            "minimum": 1,
            "maximum": 50000,
            "default": 25000,
            "description": "Maximum number of tokens to generate"
        },
        "do_sample": {
            "type": "boolean",
            "default": false,
            "description": "Whether to use sampling (true) or greedy decoding (False)"
        },
        "temperature": {
            "type": "number",
            "minimum": 0.0,
            "maximum": 2.0,
            "default": 1.0,
            "description": "Temperature for sampling (only used when do_sample=true)"
        },
        "top_p": {
            "type": "number",
            "minimum": 0.0,
            "maximum": 1.0,
            "default": 0.95,
            "description": "Top-p (nucleus) sampling parameter (only used when do_sample=true)"
        },
        "streaming": {
            "type": "boolean",
            "default": false,
            "description": "Enable streaming output (yields partial results as they're generated)"
        },
        "trust_remote_code": {
            "type": "boolean",
            "default": false,
            "description": "Whether to trust remote code when loading the model"
        },
        "cache_dir": {
            "type": [
                "string",
                "null"
            ],
            "default": null,
            "description": "Directory to cache downloaded models"
        },
        "compile_model": {
            "type": "boolean",
            "default": false,
            "description": "Use torch.compile for potential speedup (requires PyTorch 2.0+)"
        },
        "load_in_8bit": {
            "type": "boolean",
            "default": false,
            "description": "Load model in 8-bit quantization (requires bitsandbytes)"
        },
        "load_in_4bit": {
            "type": "boolean",
            "default": false,
            "description": "Load model in 4-bit quantization (requires bitsandbytes)"
        }
    },
    "required": [
        "model_id"
    ],
    "additionalProperties": false
}